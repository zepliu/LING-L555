{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aaea676",
   "metadata": {},
   "source": [
    "## L555  Final Project\n",
    "\n",
    "Zeping Liu (zepliu@iu.edu) \n",
    "\n",
    "StartDate: November 17, 2021  \n",
    "EndDate: December 8, 2021\n",
    "\n",
    "\n",
    "The aim of this program is **to obtain key measurements of sentence complexity in elementary school instructional materials from Grade 1 to Grade 6 used in mainland China**. Here, we identified two major measures to conceptualize sentence complexity from applied linguistic studies (Ortega, 2003; Kreyer, 2006; Cheung & Kemper, 1992; Lu, 2010). \n",
    "\n",
    "1. Length of sentences in terms of words and characters; \n",
    "2. Syntactic complexity:\n",
    "    - The depth of syntactic trees;\n",
    "    - The amount of common relative clauses;\n",
    "    - The dependency distance.\n",
    "    \n",
    "References:\n",
    "<br>\n",
    "[1] Cheung, H. & Kemper, S. (1992). “Competing complexity metrics and adults’ production of complex sentences”. Applied Psycholinguistics, 13 (1), 53-76.\n",
    "<br>\n",
    "[2] Kreyer, R. (2006). Inversion in Modern Written English: Syntactic Complexity, Information Status and the Creative Writer. Tübingen: Gunter Narr Verlag.\n",
    "<br>\n",
    "[3] Lu, X. (2010). Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4), 474-496.\n",
    "<br>\n",
    "[4] Ortega, L. (2003). Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college-level L2 writing. Applied Linguistics, 24: 492–518.\n",
    "<br>\n",
    "[5] Xia, F. (2000). The part-of-speech tagging guidelines for the Penn Chinese Treebank (3.0). IRCS Technical Reports Series, 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CoreNLP\n",
    "corenlp_path = '/Users/zeping/My Drive/Courses/Fall 2021/Programming for Computational Linguistics/Files/LING-L555/stanford-corenlp'\n",
    "# stanza.install_corenlp(dir=corenlp_path)\n",
    "\n",
    "### download chinese models: latest is 4.2.0\n",
    "# stanza.download_corenlp_models(model='chinese', version='4.3.2', dir=corenlp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import packages\n",
    "\n",
    "from stanza.server import CoreNLPClient\n",
    "import os, re\n",
    "\n",
    "### Set CORENLP_HOME so the computer knows where to look for CoreNLP \n",
    "\n",
    "os.environ[\"CORENLP_HOME\"] = corenlp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a client object\n",
    "client = CoreNLPClient(\n",
    "    properties='chinese',\n",
    "    memory='4G', \n",
    "    # annotators describe which tool(s) you want to use to process the input sentences\n",
    "    # see full list: https://stanfordnlp.github.io/CoreNLP/annotators.html\n",
    "    annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner', 'parse'],\n",
    "    endpoint='http://localhost:9002',\n",
    "    # json is easier to get the parse tree as a string\n",
    "    output_format='json',\n",
    "    # prints out the log\n",
    "    be_quiet=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fcb10f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a simple sentence\n",
    "\n",
    "text = '''\n",
    "小壁虎爬呀爬，爬到小河边。他看见小鱼摇着尾巴，在河里游来游去。小壁虎说：“小鱼姐姐，您把尾巴借给我行吗？”小鱼说：“不行啊，如果我没有尾巴，我就没有办法拨水了。”\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d971064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# annotation happens here\n",
    "document = client.annotate(text)\n",
    "# `document` (a python dictionary) stores all the results\n",
    "\n",
    "print(\"{:12s}\\t{:12s}\\t{:6s}\\t{}\".format(\"Word\", \"Lemma\", \"POS\", \"NER\"))\n",
    "print('-'*50)\n",
    "print()\n",
    "\n",
    "############# 1. Length #############\n",
    "\n",
    "# Count Chinese words\n",
    "D = []  # Tokenization for dependency distance analysis\n",
    "sum_t = 0\n",
    "for i, sent in enumerate(document['sentences']):\n",
    "    print(\"[Sentence {}]\".format(i+1))\n",
    "    d = [t['word'] for t in sent['tokens']]  \n",
    "    D.append(d)    \n",
    "    for t in sent['tokens']:\n",
    "        print(\"{:12s}\\t{:12s}\\t{:6s}\\t{}\".format(t['word'], t['lemma'], t['pos'], t['ner']))\n",
    "    # To get the number of tokens excluding punctuations\n",
    "    t_length = len([d for d in sent['tokens'] if ('pos', 'PU') not in d.items()])\n",
    "    print(\"Number of tokens is\", t_length)\n",
    "    print()\n",
    "    sum_t = sum_t + t_length\n",
    "nSent = len(document['sentences'])\n",
    "\n",
    "# Count Chinese characters\n",
    "def hans_count(str):\n",
    "    hans_total = 0\n",
    "    for s in str:\n",
    "        if '\\u4e00' <= s <= '\\u9fef':\n",
    "            hans_total += 1\n",
    "    return hans_total\n",
    "    \n",
    "print('='*50)\n",
    "print(\"Total number of sentences is\", nSent)\n",
    "print(\"Total number of tokens is\", sum_t)\n",
    "print(\"Total number of characters is\", hans_count(text))\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 2.1: Depth of syntactic trees #############\n",
    "\n",
    "trees_str = []\n",
    "for i, sent in enumerate(document['sentences']):\n",
    "    print(\"[Sentence {}]\".format(i+1))\n",
    "    print(sent['parse'])\n",
    "    # save the trees in bracketed format\n",
    "    tbf = re.sub('\\n\\s+', ' ', sent['parse'])\n",
    "    trees_str.append(tbf)\n",
    "    print()\n",
    "# print(trees_str)\n",
    "\n",
    "# convert to nltk tree\n",
    "# using: nltk.Tree.fromstring()\n",
    "trees_nltk = []\n",
    "for i, tree_str in enumerate(trees_str):\n",
    "    tree_nltk = nltk.Tree.fromstring(tree_str)\n",
    "    trees_nltk.append(tree_nltk)\n",
    "print(f'we have {len(trees_nltk)} trees')\n",
    "print()\n",
    "\n",
    "# let's compute the depth for each trees \n",
    "total_sd = 0\n",
    "for k in trees_nltk:\n",
    "    k.pretty_print()\n",
    "    kh = k.height()\n",
    "    total_sd = total_sd + kh\n",
    "    print(\"The syntactic depth is:\", kh)\n",
    "    print()\n",
    "    \n",
    "print('='*50)\n",
    "print(\"Total number of syntactic depth is\", total_sd)\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd262eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 2.2: Amount of Relative Clauses #############\n",
    "nRC = 0\n",
    "\n",
    "# looking for CSs\n",
    "for i, sent in enumerate(document['sentences']):\n",
    "    for w in sent['tokens']:\n",
    "        if w['pos'] == 'DEC':\n",
    "            nRC = nRC + 1\n",
    "print(\"The amount of relative clauses is:\", nRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4dc10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############# 2.3: Dependency distance #############\n",
    "\n",
    "nlp_zh = stanza.Pipeline(\"zh-hans\", processors=\"lemma, tokenize, pos, depparse\", tokenize_pretokenized=True) \n",
    "\n",
    "# D stores the tokenization\n",
    "tree = nlp_zh(D).sentences\n",
    "odd = 0\n",
    "for t in tree:\n",
    "    dd_sum = 0\n",
    "    tdnew = [d for d in t.dependencies if d[1] != 'punct']   # Punctuations are not taken into account\n",
    "    dep_num = len(tdnew)\n",
    "    for dep in tdnew:\n",
    "        dd = int(dep[2].id)-dep[2].head\n",
    "        dd_sum = dd_sum + abs(dd)\n",
    "        ave_dd = dd_sum/dep_num\n",
    "        print(dep[0].text, dep[1], dep[2].text, dd)\n",
    "    odd = odd + ave_dd\n",
    "    print(nSent)\n",
    "    print(\"The dependency distance of this sentence is:\", ave_dd)\n",
    "    print()\n",
    "print(\"The overall dependecy distance of this text is:\", round(odd/nSent,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbfeeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "################################ Apply these processes for each text in each grade level############################\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59084284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the files\n",
    "from pathlib import Path\n",
    "p = Path('/Users/zeping/My Drive/Courses/Fall 2021/Programming for Computational Linguistics/Files/LING-L555/Text/Grade1_A')\n",
    "\n",
    "texts = {}\n",
    "for fn in p.glob(\"*.txt\"):\n",
    "    if fn not in texts:\n",
    "        texts[fn] = [open(fn).read(), {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83916158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a function to process all texts\n",
    "\n",
    "def get_stats(text):\n",
    "    # annotation happens here\n",
    "    document = client.annotate(text)\n",
    "    # Aim 1: Length for sentences / words / characters\n",
    "    sum_t = 0\n",
    "    D = []\n",
    "    for i, sent in enumerate(document['sentences']):\n",
    "        d = [t['word'] for t in sent['tokens']]  \n",
    "        D.append(d) \n",
    "        for t in sent['tokens']:\n",
    "            t_length = len([d for d in sent['tokens'] if ('pos', 'PU') not in d.items()])\n",
    "        sum_t = sum_t + t_length\n",
    "    nSent = len(document['sentences'])\n",
    "    def hans_count(str):\n",
    "        hans_total = 0\n",
    "        for s in str:\n",
    "            if '\\u4e00' <= s <= '\\u9fef':\n",
    "                hans_total += 1\n",
    "        return hans_total\n",
    "    # Aim 2: Depth of syntactic trees\n",
    "    trees_str = []\n",
    "    for i, sent in enumerate(document['sentences']):\n",
    "        tbf = re.sub('\\n\\s+', ' ', sent['parse'])\n",
    "        trees_str.append(tbf)\n",
    "    trees_nltk = []\n",
    "    for i, tree_str in enumerate(trees_str):\n",
    "        tree_nltk = nltk.Tree.fromstring(tree_str)\n",
    "        trees_nltk.append(tree_nltk)\n",
    "    total_sd = 0\n",
    "    for k in trees_nltk:\n",
    "        kh = k.height()\n",
    "        total_sd = total_sd + kh\n",
    "    # Aim 3: The number of relative clauses by searching relativizers\n",
    "    nRC = 0\n",
    "    for i, sent in enumerate(document['sentences']):\n",
    "        for w in sent['tokens']:\n",
    "            if w['pos'] == 'CS':\n",
    "                nRC = nRC + 1\n",
    "    # Aim 4: Dependency distance\n",
    "    nlp_zh = stanza.Pipeline(\"zh-hans\", processors=\"lemma, tokenize, pos, depparse\", tokenize_pretokenized=True) \n",
    "    tree = nlp_zh(D).sentences\n",
    "    overall_dd = 0\n",
    "    for t in tree:\n",
    "        dd_sum = 0\n",
    "        tdnew = [d for d in t.dependencies if d[1] != 'punct']   # Punctuations are not taken into account\n",
    "        dep_num = len(tdnew)\n",
    "        for dep in tdnew:\n",
    "            dd = int(dep[2].id)-dep[2].head\n",
    "            dd_sum = dd_sum + abs(dd)\n",
    "            ave_dd = dd_sum/dep_num\n",
    "        overall_dd = overall_dd + ave_dd\n",
    "        \n",
    "    \n",
    "    # Print out all things\n",
    "    return {'nSent': nSent,\n",
    "           'nWord': sum_t,\n",
    "           'nCha': hans_count(text),\n",
    "           'sDepth': total_sd,\n",
    "           'nRC': nRC,\n",
    "           'DD': round(overall_dd/nSent,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153edd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 3: Get the statistics for each text:\n",
    "\n",
    "for text in texts:\n",
    "    texts[text][1] = get_stats(texts[text][0])\n",
    "    \n",
    "textlist = texts.items()\n",
    "textlist = list(textlist)\n",
    "textlist.sort()\n",
    "outfile = 'results.tsv'\n",
    "outfilefd = open(outfile, 'w')\n",
    "for (fn, sent) in textlist:\n",
    "    filename = fn.name.split(\"/\")[-1]\n",
    "    #print(filename, sent[1])\n",
    "    #print('%s\\t%s'%(filename,'\\t'.join(['%.2f'% i for i in sent[1].items()])))\n",
    "    print('%s\\t%d\\t%d\\t%d\\t%d\\t%d\\t%d\\t.2f' % (filename, sent[1]['nSent'], sent[1]['nWord'], sent[1]['nCha'], sent[1]['sDepth'], sent[1]['nRC'], sent[1]['DD']),file=outfilefd)\n",
    "outfilefd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd132ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the output files\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merry Christmas\n",
    "print('\\n'.join([' ' * 10 + '*' + ' ' * 10, ' ' + \"* Merry Christmas *\",\n",
    "                 '\\n'.join('{0}{1}{0}'.format(' ' * ((21 - c) // 2), \n",
    "                                              ''.join(map(lambda i: '#' if i % 2 else 'o', range(c)))) for c in range(3, 22, 2)), \n",
    "                 ' ' * 9 + '/|\\\\' + ' ' * 9]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
